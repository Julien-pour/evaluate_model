print("aaaaaaa")


import argparse
import copy
import os
# from key import wandb_key   
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM,TrainingArguments,LlamaTokenizer

from trl import SFTTrainer, DataCollatorForCompletionOnlyLM
from tqdm import tqdm
import json
from datasets import Dataset
import portalocker
import random
from time import sleep

# from peft import LoraConfig
import numpy as np

from utils_test import return_full_prompt,prompt_train,pass_at_k,judge_parallel,modify_docstring

parser = argparse.ArgumentParser(description="Example script for argument parsing")
parser.add_argument("-z", "--base_path", type=str, help="path to git project evaluate_model",default="/media/data/flowers/evaluate_model/")
parser.add_argument("--path_model_base", type=str, help="path where hf model are saved",default="/gpfsscratch/rech/imi/uqv82bm/hf/")
parser.add_argument("--path_archive", type=str, help="path where archive for training is",default="/gpfsscratch/rech/imi/uqv82bm/hf/")

# parser.add_argument("-p", "--path_dir", type=str, help="path to evaluate_model")

parser.add_argument("-p", "--arg_path_idx", type=int, help="path baseline idx  (data to use as trainset)",default=0)
parser.add_argument("-e", "--arg_epoch", type=int, help="number epoch",default=2)
parser.add_argument("-b", "--arg_bs", type=int, help=" bs train",default=1)
parser.add_argument("-c", "--arg_bs_test", type=int, help=" bs test",default=64)

parser.add_argument("-m", "--arg_model_id", type=str, help=" model",default="deepseek-coder-1.3b-instruct")
parser.add_argument("-s", "--arg_seed", type=str, help="seed ",default="1")
parser.add_argument("-t", "--arg_mode", type=str, help=" just train, just eval or train+eval ['train_eval','eval','train']",default="train_eval")
parser.add_argument("-k", "--arg_k", type=int, help="k in pass@k",default=10)
parser.add_argument("-g", "--arg_gpu", type=str, help="GPU use",default="v100")
parser.add_argument("-a", "--accum_step", type=int, help="number of accumulation step",default=1)
parser.add_argument("--test_base_model", type=str, help="just test base model",default="False")
parser.add_argument("--test_base_model_on_train", type=str, help="just test_base_model_on_train",default="False")
parser.add_argument("--lr", type=float, help="learning rate")
parser.add_argument("--ratio", type=float, help="how much data in % to keep with the highest quality",default=1.0)
parser.add_argument("--random", type=str, help="if ratio<1. sample ratio in % of total data randomly",default="False")
parser.add_argument("--description", type=str, help="use description when training",default="False")


n_max_token=1360 #8*


args = parser.parse_args()

learning_rate= args.lr
data_ratio= float(args.ratio)
if args.arg_gpu == "v100":
    type_use = torch.float16
    bf16=False
    fp16=True
else:
    type_use = torch.bfloat16
    bf16=True
    fp16=False

accum_step=args.accum_step
# /!\ set that
# limited_trainset=True # data generated by expe with 3 first example from trainset or full trainset

name = args.path_archive.split("archives/")[1].split(".json")[0]
quality_run= "quality" in name
seed= name.split("seed-")[1]
just_name=name.split("_seed-")[0]
if "quality" in just_name:
    just_name=just_name.split("_quality")[0]
use_description = args.description == "True"
# name: name of the methode (aces,elm-nlp,aces)
params={"name":just_name, "use_description":use_description,"lr":args.lr,"data_ratio":data_ratio,
        "accum_step":accum_step, "batch_size": args.arg_bs,"data_ratio":data_ratio,
        "random_sampling":args.random,"epochs":args.arg_epoch,"seed":args.arg_seed,
        "quality_run":quality_run,"seed":seed}

unique_id=f"{os.getenv('SLURM_ARRAY_JOB_ID')}_{os.getenv('SLURM_ARRAY_TASK_ID')}"
filename_save = args.base_path+"save_results/multiple_results/"+f"results_{unique_id}.json"

if args.arg_mode=="train_eval":
    train_model=True
    eval_model=True
    
if args.arg_mode=="train":
    train_model=True
    eval_model=False 
if args.arg_mode=="eval":
    train_model=False
    eval_model=True 

if args.test_base_model=="True":
    train_model=False
    eval_model=True 

if args.arg_epoch:
    num_train_epochs= args.arg_epoch

seed = str(args.arg_seed)



# test model

os.environ["WANDB_DISABLED"] = "True"

os.environ['TRANSFORMERS_OFFLINE'] = "1"
os.environ['WANDB_MODE'] = "offline"
os.environ["WANDB_PROJECT"] = "codegpt finetuned"
# os.environ['WANDB_API_KEY'] = wandb_key
os.environ['WANDB_CACHE_DIR'] = args.base_path+"wandb_cache/"


# os.environ['HF_DATASETS_CACHE'] = args.base_path+"hf/datasets"
# os.environ['TRANSFORMERS_CACHE'] = args.base_path+"hf/models"

os.environ['TOKENIZERS_PARALLELISM'] = "True"


path_train_idx = args.arg_path_idx

# define all path
path_train = args.base_path 
path_train += args.path_archive
#list_name[path_train_idx]

path_save=args.base_path+"save_results/"+name+".json"
print("\n=============\n ")

print("path train ",path_train)
print("\n=============\n ")

run_name_wandb=path_train.split("archives/")[1].split("/")[0]
run_name_wandb = run_name_wandb.replace("/","_")
print(path_train)


#path model

model_id =   args.arg_model_id

hf_dir=args.path_model_base

path_load_model=hf_dir+model_id
print("path_load_model",path_load_model)

# hf way to load json dataset
with open(path_train, encoding="utf-8") as f:
    dataset = json.load(f)


to_remove=["emb","target_skills","puzzle_history","quality","is_valid","is_valid_explanation"]
for i in dataset:
    # if args.test_base_model_on_train=="True" and i["idx_generation"]!=-1:
    #     del i
    #     continue
    for j in to_remove:
        if j in i:
            del i[j]
from datasets import Dataset

def formatting_prompts(example,prompt_solve_puzzle=prompt_train,use_description=use_description):
    """formatting function for training"""
    output_texts = []
    # print(len(example['program_str']))
    for i in range(len(example)):

        puzzle= example[i]['program_str']
        try:
            prompt_f=puzzle.split("def g(")[0]
            prompt_g= "def g(" + puzzle.split("def g(")[1]
            if use_description:
                prompt_f=modify_docstring(prompt_f, "f", example[i]['description'])

            full_prompt = prompt_solve_puzzle.format(pb=prompt_f,g=prompt_g)#,g_firstline=prompt_g)
            output_texts.append(full_prompt)
        except Exception as e:
            print("error: ",e)
            print("error in formatting_prompts_func idx",i)
            print(example[i]['program_str'])
            print("======================")
            print(puzzle)
    return output_texts

out_dataset=formatting_prompts(dataset)

def formatting_prompts_func(example,prompt_solve_puzzle=prompt_train,use_description=use_description):
    """formatting function for training 
     ====== idk why the index need to be in reverse order ======
     """
    output_texts = []
    # print(len(example['program_str']))
    for i in range(len(example['program_str'])):

        puzzle= example['program_str'][i]
        try:
            prompt_f=puzzle.split("def g(")[0]
            prompt_g= "def g(" + puzzle.split("def g(")[1]
            if use_description:
                prompt_f=modify_docstring(prompt_f, "f", example['description'][i])

            full_prompt = prompt_solve_puzzle.format(pb=prompt_f,g=prompt_g)#,g_firstline=prompt_g)
            output_texts.append(full_prompt)
        except Exception as e:
            print("error: ",e)
            print("error in formatting_prompts_func idx",i)
            print(example['program_str'][i])
            print("======================")
            print(puzzle)
            raise e
    print("len output_texts",len(output_texts))
    return output_texts
tokenizer = AutoTokenizer.from_pretrained(path_load_model,local_files_only=False)
print(f"len process dataset = {len(out_dataset)} after removing data > {n_max_token} tokens")   


assert len(out_dataset)==len(dataset)
inputs = tokenizer(out_dataset, padding=False)  # maybe need to batch that
list_tok = [len(inputs.input_ids[i]) for i in range(len(inputs.input_ids))]

to_remove = np.array(list_tok) >= n_max_token-100 #100 for prompt

sorted_data = sorted(dataset, key=lambda x: x['fitness'], reverse=True)
sorted_data = [data for data, remove in zip(dataset, to_remove) if not remove]

# keep only the data points where to_remove is False
if args.random=="True":
    # shuffle data
    random.shuffle(sorted_data)

if abs(data_ratio-1.0)<1e-3:
    dataset = sorted_data
else:
    dataset = sorted_data[:int(len(sorted_data)*data_ratio)]
        

dataset = Dataset.from_list(dataset)

# dataset = load_dataset("json", data_files=path_train, trust_remote_code=True)#, split="train")
# dataset = load_dataset(path_train)#, trust_remote_code=True)#, split="train")



cat_datasets = dataset.shuffle(seed=42) 
print("len dataset",len(dataset))



name_json_save_all = args.base_path+f"save_results/passk_grid_search.json"#.split("/")[1]
run_name = name+unique_id
if args.test_base_model_on_train=="True":
    run_name = model_id+unique_id+"_basemodel"

if not os.path.exists(name_json_save_all):
    # Create a new JSON file with some sample data
    sample_data = {}
    with open(name_json_save_all, 'w') as file:
        json.dump(sample_data, file, indent=4)


name_json = args.base_path+"save_results/"+name+model_id#.split("/")[1]
name_json_sol = args.base_path+"save_sol/"+name+model_id#.split("/")[1]

if train_model:
    tokenizer = AutoTokenizer.from_pretrained(path_load_model,local_files_only=False)

    tokenizer.padding_side='right'
    tokenizer.pad_token = tokenizer.eos_token


    model = AutoModelForCausalLM.from_pretrained(
        path_load_model,
        # torch_dtype=type_use,
        # quantization_config=quantization_config,
        device_map="auto",
        local_files_only=True
    )
    #training
    


    lr_scheduler_type= "cosine"

    warmup_ratio=0.1

    response_template= "Solution 1:"

    # list_tok_response_template=tokenizer(response_template)["input_ids"][1:]

    collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer,mlm=False)




    run_name_wandb += "Epoch"+str(num_train_epochs)
    run_name_wandb+= model_id#.split("/")[1]

    

    training_arguments=TrainingArguments(
        per_device_train_batch_size=args.arg_bs,
        # per_device_eval_batch_size=4,
        # evaluation_strategy="steps",
        gradient_accumulation_steps=accum_step,
        run_name= run_name_wandb,
        # warmup_steps=2,
        save_strategy="no",
        warmup_ratio=warmup_ratio,
        lr_scheduler_type=lr_scheduler_type,
        # max_steps=500,
        num_train_epochs=num_train_epochs,
        # weight_decay=0.001,
        learning_rate=learning_rate,
        bf16=bf16, 
        fp16=fp16,
        # bf16_full_eval=True,
        gradient_checkpointing=False,
        logging_steps=1,
        output_dir="outputs",
        optim="adamw_torch",#"adamw_torch",#"paged_adamw_8bit",#"paged_adamw_32bit",
        max_grad_norm=0.3,
        # group_by_length=True,
        # do_eval=True,
        # eval_steps=10,
        # torch_compile=True
        
    )

    config = {"lr":learning_rate, "batch_size": args.arg_bs,"warmup_ratio":warmup_ratio,"model_name":"model_id", "epoch":args.arg_epoch}
    # config.update({"architecture": "", "depth": 34})
    # wandb.init(#name=name_wb,
    #            config=config,project="finetune llama")


    trainer = SFTTrainer(
        model,#"EleutherAI/gpt-neo-125m",
        tokenizer=tokenizer,
        train_dataset=cat_datasets,
        # eval_dataset=dataset_r["test"],
        # dataset_text_field="program_str",

        formatting_func=formatting_prompts_func,
        data_collator=collator,
        max_seq_length=n_max_token,
        args=training_arguments

    )
    trainer.train()

    output_dir = hf_dir+model_id+run_name #args.base_path+"hf/datasets"+name # where to save model
    trainer.save_model(output_dir)

if eval_model:  # OOD

    output_dir = hf_dir+model_id+run_name
    if train_model:
        trainer.accelerator.clear()
        import gc
        del model
        del tokenizer
        gc.collect()
        torch.cuda.empty_cache()
        for obj in gc.get_objects():
            if torch.is_tensor(obj):
                obj.cpu()            
        gc.collect()
        torch.cuda.empty_cache()

    if not train_model:
        import gc
        gc.collect()
        torch.cuda.empty_cache()
        for obj in gc.get_objects():
            if torch.is_tensor(obj):
                obj.cpu()            
        gc.collect()
        torch.cuda.empty_cache()


    # testing
    if args.test_base_model=="True":
        output_dir = hf_dir+model_id
        run_name = model_id+"_base"
    tokenizer = AutoTokenizer.from_pretrained(output_dir,local_files_only=True)
    model = AutoModelForCausalLM.from_pretrained(
        output_dir,
        torch_dtype=type_use,

        # quantization_config=quantization_config,
        device_map="auto",
        local_files_only=True
    )
    tokenizer.padding_side='left'
    tokenizer.pad_token = tokenizer.eos_token
    model.eval()
    model.config.use_cache = True
    # model=torch.compile(model)



    # testset= preprocessing_P3_no_test(split="test",n_token_max=1024,path=args.base_path,tokenizer=tokenizer)
    path_test=args.base_path+"P3_test_emb_wizard3B.json"#"/home/flowers/work/OpenELM/old/run/P3_test_emb_codet5p.json"
    with open(path_test,mode = "r") as f:
        testset = json.load(f)
    curr_idx=0
    correct_puzz=0

    num_return_sequences=args.arg_k #n_try
    list_all_passk=[[] for i in range(num_return_sequences)]
    list_passk=[]

    list_puzzle=[]
    list_all_puzzle=[]

        
    list_testset= [x["program_str"] for x in testset]
    list_puzzle_correct=[]

    bs = args.arg_bs_test
    with torch.inference_mode():
        
        for idx in tqdm(range(curr_idx,len(list_testset),bs)): #len(dataset["test"])
            # idx=0
            print(f"\n\n============ idx {idx} ==================\n")
            flag=True
            attempt=0
            list_puzzle_idx=[]
            list_prompt=[]
            list_prompt_f=[]
            subset_test = list_testset[idx:idx+bs]
            for idx_puz in range(len(subset_test)):
                prompt_f = subset_test[idx_puz].split("def g(")[0]
                list_prompt_f.append(prompt_f)
                prompt = return_full_prompt(model_id=model_id,pb=prompt_f) # todo
                list_prompt.append(prompt)
            inputs = tokenizer(list_prompt, return_tensors="pt",padding=True).to("cuda")
            # for idx_inp in range(len(inputs)):
            len_prompt = inputs["input_ids"].shape[1]
            list_puzzle_gen=[[] for _ in range(len(list_prompt))]
            for idx_gen in range(num_return_sequences):
                outputs = model.generate(**inputs,max_new_tokens=512,do_sample=True, temperature=0.7)
                generated_texts = tokenizer.batch_decode(outputs[:,len_prompt:], skip_special_tokens=True)
                for idx_out_gen in range(len(outputs)):
                    list_puzzle_gen[idx_out_gen].append(generated_texts[idx_out_gen])

            list_generated_text = copy.deepcopy(list_puzzle_gen)

            for i in range(len(list_puzzle_gen)): # along the bs
                dic_save={}
                list_raw_puzzle = []
                list_proc_puzzle =[]
                for j in range(len(list_puzzle_gen[i])):
                    prompt_f =list_prompt_f[i]
                    try:
                        #check if "```" is in list_puzzle_gen[i][j]
                        list_puzzle_gen[i][j] = list_puzzle_gen[i][j].replace("```python","```")
                        list_puzzle_gen[i][j] = list_puzzle_gen[i][j].replace("```Python","```")

                        if "```" in list_puzzle_gen[i][j]:
                            extract_g=list_puzzle_gen[i][j].split("```")[1].split("assert")[0]
                        else:
                            if "assert" in list_puzzle_gen[i][j]:
                                extract_g=list_puzzle_gen[i][j].split("assert")[0]
                            else:    
                                extract_g=list_puzzle_gen[i][j]
                    except:
                        print("error extract g")
                        print(list_puzzle_gen[i][j])
                    extract_g = extract_g+"\nassert f(g()) == True\n"
                    test_fg= prompt_f+extract_g 
                    list_puzzle_gen[i][j] = test_fg
                    list_puzzle.append(test_fg)
                    list_proc_puzzle.append(test_fg)
                    list_raw_puzzle.append(prompt_f+list_puzzle_gen[i][j])
                dic_save["raw_puzzle"]=list_raw_puzzle
                dic_save["process_puzzle"]=list_proc_puzzle
                
                    # if j<1:
                    #     print("\n-------------------\n")
                    #     print(test_fg)
                    
                
                list_valid_puzzles = judge_parallel(list_puzzle_gen[i])
                dic_save["list_valid"]=list_valid_puzzles                 
                list_all_puzzle.append(dic_save)    

                cor_puz= np.sum(list_valid_puzzles)

                n_sample, n_correct=num_return_sequences,cor_puz
                pass_k = pass_at_k(n_sample, n_correct, k=num_return_sequences)
                list_passk.append(pass_k)
                #compute passk for k=[1,...,num_return_sequences]
                for idx_passk in range(num_return_sequences):
                    pass2add=pass_at_k(n_sample, n_correct, k=idx_passk+1)
                    list_all_passk[idx_passk].append(pass2add)
                    testset[idx + i][f'pass_{idx_passk+1}'] = pass2add



                proba_solved = n_correct / n_sample
                testset[idx + i]['proba_solved'] = float(proba_solved)
                testset[idx + i]['n_sample'] = int(n_sample)
                testset[idx + i]['n_correct'] = int(n_correct)
                testset[idx + i]['generated_text'] = list_generated_text[i]
                testset[idx + i]['parsed_puzzles'] = list_puzzle_gen[i]
                # testset[idx + i]['prompt'] = list_prompt[i]

                
            print(f"correct puzzles: {int(np.sum(list_passk))}/{len(list_passk)}")
            # with open(name_json+".json", "w") as outfile:
            #     json.dump(list_passk,outfile)

        for idx_passk in range(num_return_sequences):
            print(f"pass {idx_passk+1}: {np.sum(list_all_passk[idx_passk])}/{len(list_all_passk[idx_passk])}")
        dic_passk={}
        for idx_passk in range(num_return_sequences):
            dic_passk[f"pass_{idx_passk+1}"]=float(np.sum(list_all_passk[idx_passk]))

        final_results = {
            'parameters': params,
            'results': dic_passk
        }

        n_try=0
        while n_try<30:
            n_try+=1
            sleeptime = random.uniform(1, 30)
            print("sleeping for:", sleeptime, "seconds")
            sleep(sleeptime)

            try:
                with open(name_json_save_all, "r+") as outfile:
                    portalocker.lock(outfile, portalocker.LOCK_EX)  # Lock the file for exclusive writing
                    json_content=json.load(outfile)
                    if just_name not in json_content:
                        json_content[just_name]=[]
                    json_content[just_name].append(final_results) 
                    outfile.seek(0)
                    json.dump(json_content, outfile,indent=4)
                    outfile.truncate()  # Truncate file size in case new data is smaller
                    portalocker.unlock(outfile)
                    n_try=30
                    break


            except:
                pass
        # with open(name_json_save_all, "r") as outfile:
        #     json_content=json.load(outfile)
        # json_content[run_name]=dic_passk 
        # with open(name_json_save_all, "w") as outfile:
        #     json.dump(json_content,outfile,indent=4)


        # with open(name_json+"_e"+str(num_train_epochs)+"_seed_"+seed+".json", "w") as outfile:
        #     json.dump(json_content,outfile,indent=4)
        # with open(name_json_sol+"_e"+str(num_train_epochs)+"_seed_"+seed+".json", "w") as outfile:
        #     json.dump(list_all_puzzle,outfile,indent=4)
        
        with open(filename_save, 'w') as f:
            json.dump(final_results, f, indent=4)

        # with open(name_json_sol+"_e"+str(num_train_epochs)+"_seed_"+seed+".json", "w") as outfile:
        #     json.dump(testset,outfile,indent=4 )



